{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM-base-language-model\n",
    "\n",
    "- Model:\n",
    "- Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import preprocess_helper\n",
    "import embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @TODO define a class Model\n",
    "class Model():\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.batch_size = 64\n",
    "    self.epoch = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @TODO lines that will be used to define the Model \n",
    "\n",
    "# Define the model in Tensorflow ...\n",
    "\n",
    "sentence_length = 30        # number of words per sentence (ie. how many times we should enroll the network)\n",
    "batch_size = 64             # number of sentences analysed per batch \n",
    "embedding_dimensions = 100  # dimension of the embedding \n",
    "state_size = 512            # dimension of the hidden state \n",
    "max_grad_norm = 10          # max norm of the gradient \n",
    "vocabulary_size = 20000     # vocabulary size \n",
    "\n",
    "embedding = np.random.uniform(low=0, high=1, size=embedding_dimensions)\n",
    "\n",
    "# place holder for a batch dim= 30 x 64 x 100\n",
    "batch = tf.placeholder(tf.float32, [sentence_length, batch_size, embedding_dimensions])\n",
    "\n",
    "# var for the embedding of dim=20k x 100\n",
    "embedding = tf.get_variable(\"embedding\", [vocabulary_size, embedding_dimensions], dtype=tf.float32)\n",
    "\n",
    "# this i don't know what it is\n",
    "inputs = tf.nn.embedding_lookup(embedding,\n",
    "                                input_data)\n",
    "\n",
    "# construct basic LSTM cell \n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "# Initial state of the LSTM memory.\n",
    "hidden_state = tf.zeros([batch_size, lstm.state_size])\n",
    "current_state = tf.zeros([batch_size, lstm.state_size])\n",
    "state = hidden_state, current_state\n",
    "probabilities = []\n",
    "loss = 0.0\n",
    "\n",
    "# loop over each word in the batch of sentences \n",
    "for current_words in batch:\n",
    "  \n",
    "    # The value of state is updated after processing each batch of words.\n",
    "    output, state = lstm(current_words, state)\n",
    "\n",
    "    # The LSTM output can be used to make next word predictions\n",
    "    logits = tf.matmul(output, weights) + bias\n",
    "    probabilities.append(tf.nn.softmax(logits))\n",
    "    loss += tf.losses.sparse_softmax_cross_entropy(\n",
    "                labels,\n",
    "                logits,\n",
    "                weights=1.0,\n",
    "                scope=None,\n",
    "                loss_collection=tf.GraphKeys.LOSSES,\n",
    "                reduction=Reduction.SUM_BY_NONZERO_WEIGHTS)\n",
    "    \n",
    "    cost = tf.reduce_sum(loss)\n",
    "    \n",
    "# once the loss was calculated, backprop to update the weights \n",
    "learning_rate = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                  max_grad_norm)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(self._lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simplified version of the Tensorflow tutorial on RNNs\n",
    "# available at: https://www.tensorflow.org/tutorials/recurrent\n",
    "\n",
    "def main():\n",
    "\n",
    "  frequent_words = load_frequent_words('google-10000-english/20k.txt')\n",
    "  test_data = load_and_process_data('data/sentences.eval', frequent_words, 28)\n",
    "  train_data = load_and_process_data('data/sentences.train', frequent_words, 28)\n",
    "\n",
    "  config = get_config()\n",
    "  eval_config = get_config()\n",
    "  eval_config.batch_size = 1\n",
    "  eval_config.num_steps = 1\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                                                config.init_scale)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "      train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "      with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(is_training=True, config=config, input_=train_input)\n",
    "      tf.summary.scalar(\"Training Loss\", m.cost)\n",
    "      tf.summary.scalar(\"Learning Rate\", m.lr)\n",
    "\n",
    "    with tf.name_scope(\"Valid\"):\n",
    "      valid_input = PTBInput(config=config, data=valid_data, name=\"ValidInput\")\n",
    "      with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(is_training=False, config=config, input_=valid_input)\n",
    "      tf.summary.scalar(\"Validation Loss\", mvalid.cost)\n",
    "\n",
    "    with tf.name_scope(\"Test\"):\n",
    "      test_input = PTBInput(\n",
    "          config=eval_config, data=test_data, name=\"TestInput\")\n",
    "      with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        mtest = PTBModel(is_training=False, config=eval_config,\n",
    "                         input_=test_input)\n",
    "\n",
    "    models = {\"Train\": m, \"Valid\": mvalid, \"Test\": mtest}\n",
    "    for name, model in models.items():\n",
    "      model.export_ops(name)\n",
    "    metagraph = tf.train.export_meta_graph()\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    tf.train.import_meta_graph(metagraph)\n",
    "    for model in models.values():\n",
    "      model.import_ops()\n",
    "\n",
    "    for i in range(config.max_max_epoch):\n",
    "      lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)\n",
    "      m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "      print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "      train_perplexity = run_epoch(session, m, eval_op=m.train_op,\n",
    "                                   verbose=True)\n",
    "      print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "      valid_perplexity = run_epoch(session, mvalid)\n",
    "      print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "    test_perplexity = run_epoch(session, mtest)\n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
