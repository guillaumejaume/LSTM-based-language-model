{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM-base-language-model\n",
    "\n",
    "- Model:\n",
    "- Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import time \n",
    "import json\n",
    "\n",
    "import gensim\n",
    "\n",
    "import preprocess_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load the embedding and to enroll the network (needed to execute the main())\n",
    "\n",
    "def load_embedding(session, vocab, emb, config):\n",
    "  '''\n",
    "    session        Tensorflow session object\n",
    "    vocab          A dictionary mapping token strings to vocabulary IDs\n",
    "    emb            Embedding tensor of shape vocabulary_size x dim_embedding\n",
    "    config         Config object that contains the path to embedding file and the \n",
    "                   dimensionality of the external embedding.\n",
    "  '''\n",
    "  print(\"Loading external embeddings from %s\" % config.path_to_word2vec)\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format(config.path_to_word2vec, binary=False)\n",
    "  external_embedding = np.zeros(shape=(config.vocabulary_size, config.embedding_dimensions))\n",
    "  matches = 0\n",
    "  for tok, idx in vocab.items():\n",
    "    if config.use_word2vec_emb and tok in model.vocab:\n",
    "      external_embedding[idx] = model[tok]\n",
    "      matches += 1\n",
    "    else:\n",
    "      if config.verbose:\n",
    "        print(\"%s not in embedding file\" % tok)\n",
    "      external_embedding[idx] = np.random.uniform(low=-0.25, high=0.25, size=config.embedding_dimensions)\n",
    "  \n",
    "  if config.use_word2vec_emb:\n",
    "    print(\"%d words out of %d could be loaded\" % (matches, config.vocabulary_size))\n",
    "  else:\n",
    "    print(\"Generated embedding for %d words\" % config.vocabulary_size)\n",
    "\n",
    "  pretrained_embeddings = tf.placeholder(tf.float32,\n",
    "                                         [None, None],\n",
    "                                         name='pretrained_embedding')\n",
    "  assign_op = emb.assign(pretrained_embeddings)\n",
    "  session.run(assign_op, {pretrained_embeddings: external_embedding})\n",
    "  \n",
    "def build_lstm_graph(lstm_cell, input_batch, config):\n",
    "  # cell: tensorflow LSTM object\n",
    "  # batch: tensorflow of shape batch_size x sent_len x emb_dim\n",
    "  \n",
    "  # return value is a N-D tensor of shape [batch_size, state_size] filled with zeros.\n",
    "  initial_state = lstm_cell.zero_state(config.batch_size, tf.float32)\n",
    "  \n",
    "  # init state is the init one \n",
    "  state = initial_state\n",
    "\n",
    "  # where to store the cell_output after each time_step \n",
    "  output = []\n",
    "  for time_step in range(config.sentence_length-1):\n",
    "      cell_output, state = lstm_cell(input_batch[:, time_step, :], state)\n",
    "      output.append(cell_output)\n",
    "  output = tf.reshape(output, [config.predicted_words * config.batch_size, config.state_size])  \n",
    "  return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class Config \n",
    "class Config():\n",
    "  \n",
    "  def __init__(self, path_to_config_file=None):\n",
    "    if path_to_config_file:\n",
    "      self.read(path_to_config_file)\n",
    "    else:\n",
    "      # default constructor \n",
    "      self.is_training = True          # define if the code is run for training or testing \n",
    "      self.sentence_length = 30        # number of words per sentence (ie. how many times we should enroll the network)\n",
    "      self.predicted_words = self.sentence_length - 1# number of predicted words per sentence = sentence_length - 1\n",
    "      self.batch_size = 64             # number of sentences analysed per batch \n",
    "      self.embedding_dimensions = 100  # dimension of the embedding \n",
    "      self.state_size = 512            # dimension of the hidden state \n",
    "      self.max_grad_norm = 5           # max norm of the gradient \n",
    "      self.vocabulary_size = 20004     # vocabulary size \n",
    "      self.number_of_epochs = 20       # number of epochs used during training \n",
    "      self.learning_rate = 0.1         # learning rate \n",
    "      self.path_to_word2vec =  'wordembeddings-dim100.word2vec' # path to word2vec model \n",
    "      self.use_word2vec_emb = False    # if training is done with Word2Vec or with a rand emb\n",
    "      self.verbose = False             # simple verbose param to follow training \n",
    "      self.save_model = True           # if we should save the model after training \n",
    "      self.saving_path = 'model/toy-model.ckpt'\n",
    "      self.restored_model = ''         # path where the model was saved to restore it and test it \n",
    "      self.input_file_name = 'data/sentences.eval'\n",
    "      self.frequent_words_file_name = 'data/k_frequent_words.txt'\n",
    "      \n",
    "  def read(self, path_to_config_file):\n",
    "    with open(path_to_config_file) as data_file:    \n",
    "      raw = json.load(data_file)\n",
    "      \n",
    "      self.is_training = raw['is_training']\n",
    "      self.sentence_length = raw['sentence_length']\n",
    "      self.predicted_words = raw['predicted_words']\n",
    "      self.batch_size = raw['batch_size']\n",
    "      self.embedding_dimensions = raw['embedding_dimensions']\n",
    "      self.state_size = raw['state_size']\n",
    "      self.max_grad_norm = raw['max_grad_norm']\n",
    "      self.vocabulary_size = raw['vocabulary_size']\n",
    "      self.number_of_epochs = raw['number_of_epochs']\n",
    "      self.learning_rate = raw['learning_rate']\n",
    "      self.path_to_word2vec = raw['path_to_word2vec']\n",
    "      self.use_word2vec_emb = raw['use_word2vec_emb']\n",
    "      self.verbose = raw['verbose']\n",
    "      self.save_model = raw['save_model']\n",
    "      self.saving_path = raw['saving_path']\n",
    "      self.restored_model = raw['restored_model']     \n",
    "      self.input_file_name = raw['input_file_name']\n",
    "      self.frequent_words_file_name = raw['frequent_words_file_name']\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "  \n",
    "  def __init__(self, config):\n",
    "\n",
    "    inputs = tf.placeholder(dtype=tf.float32,\n",
    "                            shape=[config.batch_size, config.sentence_length, config.embedding_dimensions],\n",
    "                            name='inputs')\n",
    "    \n",
    "    #print('inputs shape: ', inputs.shape)\n",
    "    \n",
    "    labels = tf.placeholder(dtype=tf.int32,\n",
    "                            shape=[config.batch_size, config.predicted_words],\n",
    "                            name='labels')\n",
    "\n",
    "    # construct basic LSTM cell \n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(config.state_size)\n",
    "    \n",
    "    # enroll the network (state = final_state)\n",
    "    output, state = build_lstm_graph(lstm_cell, inputs, config)\n",
    "        \n",
    "    # project state size on the vocab size dim = state_size x vocabulary_size \n",
    "    weights = tf.get_variable(\"weights\",\n",
    "                              [config.state_size, config.vocabulary_size],\n",
    "                              dtype=tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    # add a bias dim = vocabulary_size \n",
    "    bias = tf.get_variable(\"bias\",\n",
    "                           [config.vocabulary_size],\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    # compute the logits \n",
    "    logits = tf.matmul(output, weights) + bias\n",
    "    logits = tf.reshape(logits, [config.batch_size,\n",
    "                                 config.predicted_words,\n",
    "                                 config.vocabulary_size])\n",
    "    \n",
    "    # define proba with softmax layer with dim = batch_size x num_steps x vocabulary_size \n",
    "    self.probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    if not config.is_training:\n",
    "      return \n",
    "    \n",
    "    # compute the loss\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                          logits=logits)\n",
    "        \n",
    "    self.loss = tf.reduce_sum(loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=config.learning_rate)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars),\n",
    "                                      config.max_grad_norm)\n",
    "    self.train_optimizer = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                                     global_step=tf.train.get_or_create_global_step())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent_words loaded 20004\n",
      "- Number of sentences loaded:  9846\n",
      "Loading external embeddings from wordembeddings-dim100.word2vec\n",
      "Generated embedding for 20004 words\n",
      "Batch: 0\n",
      "Total loss:  18376.236328125\n",
      "Batch: 1\n",
      "Total loss:  33910.201171875\n",
      "Batch: 2\n",
      "Total loss:  68026.154296875\n",
      "Batch: 3\n",
      "Total loss:  92373.751953125\n",
      "Batch: 4\n",
      "Total loss:  151234.912109375\n",
      "Epoch:  0 with perplexity:  6945723.316975162\n",
      "Time:  6  secs\n",
      "Batch: 0\n",
      "Total loss:  45769.23828125\n",
      "Batch: 1\n",
      "Total loss:  73573.9609375\n",
      "Batch: 2\n",
      "Total loss:  94099.46875\n",
      "Batch: 3\n",
      "Total loss:  112438.16015625\n",
      "Batch: 4\n",
      "Total loss:  132370.08984375\n",
      "Epoch:  1 with perplexity:  973400.6317456958\n",
      "Time:  6  secs\n",
      "Batch: 0\n",
      "Total loss:  21142.49609375\n",
      "Batch: 1\n",
      "Total loss:  52693.86328125\n",
      "Batch: 2\n",
      "Total loss:  80984.591796875\n",
      "Batch: 3\n",
      "Total loss:  107609.72265625\n",
      "Batch: 4\n",
      "Total loss:  132317.88671875\n",
      "Epoch:  2 with perplexity:  968121.8145995481\n",
      "Time:  7  secs\n",
      "Batch: 0\n",
      "Total loss:  25227.05859375\n",
      "Batch: 1\n",
      "Total loss:  62892.9609375\n",
      "Batch: 2\n",
      "Total loss:  93706.5390625\n",
      "Batch: 3\n",
      "Total loss:  117116.609375\n",
      "Batch: 4\n",
      "Total loss:  139696.06640625\n",
      "Epoch:  3 with perplexity:  2087908.873315972\n",
      "Time:  7  secs\n",
      "Batch: 0\n",
      "Total loss:  23661.59765625\n",
      "Batch: 1\n",
      "Total loss:  55276.455078125\n",
      "Batch: 2\n",
      "Total loss:  82243.98046875\n",
      "Batch: 3\n",
      "Total loss:  103913.203125\n",
      "Batch: 4\n",
      "Total loss:  126611.603515625\n",
      "Epoch:  4 with perplexity:  534297.83160043\n",
      "Time:  6  secs\n",
      "Batch: 0\n",
      "Total loss:  23364.55859375\n",
      "Batch: 1\n",
      "Total loss:  47582.294921875\n",
      "Batch: 2\n",
      "Total loss:  73341.607421875\n",
      "Batch: 3\n",
      "Total loss:  92995.587890625\n",
      "Batch: 4\n",
      "Total loss:  113007.193359375\n",
      "Epoch:  5 with perplexity:  129518.97074103546\n",
      "Time:  6  secs\n",
      "Batch: 0\n",
      "Total loss:  20600.474609375\n",
      "Batch: 1\n",
      "Total loss:  39207.041015625\n",
      "Batch: 2\n",
      "Total loss:  58884.767578125\n",
      "Batch: 3\n",
      "Total loss:  73582.91796875\n",
      "Batch: 4\n",
      "Total loss:  88017.201171875\n",
      "Epoch:  6 with perplexity:  9589.830138880236\n",
      "Time:  6  secs\n",
      "Batch: 0\n",
      "Total loss:  13341.9697265625\n",
      "Batch: 1\n",
      "Total loss:  25336.1513671875\n",
      "Batch: 2\n",
      "Total loss:  38657.3115234375\n",
      "Batch: 3\n",
      "Total loss:  50734.6650390625\n",
      "Batch: 4\n",
      "Total loss:  60002.490234375\n",
      "Epoch:  7 with perplexity:  518.1472143210829\n",
      "Time:  6  secs\n",
      "Batch: 0\n",
      "Total loss:  9598.416015625\n",
      "Batch: 1\n",
      "Total loss:  18541.091796875\n",
      "Batch: 2\n",
      "Total loss:  29925.341796875\n",
      "Batch: 3\n",
      "Total loss:  39754.76171875\n",
      "Batch: 4\n",
      "Total loss:  49665.759765625\n",
      "Epoch:  8 with perplexity:  176.5345560600265\n",
      "Time:  7  secs\n",
      "Batch: 0\n",
      "Total loss:  8400.9462890625\n",
      "Batch: 1\n",
      "Total loss:  18152.0126953125\n",
      "Batch: 2\n",
      "Total loss:  28758.1015625\n",
      "Batch: 3\n",
      "Total loss:  37622.0185546875\n",
      "Batch: 4\n",
      "Total loss:  46884.1923828125\n",
      "Epoch:  9 with perplexity:  132.12785336080967\n",
      "Time:  7  secs\n",
      "Batch: 0\n",
      "Total loss:  8609.859375\n",
      "Batch: 1\n",
      "Total loss:  16918.19921875\n",
      "Batch: 2\n",
      "Total loss:  25666.025390625\n",
      "Batch: 3\n",
      "Total loss:  33303.5771484375\n",
      "Batch: 4\n",
      "Total loss:  42603.310546875\n",
      "Epoch:  10 with perplexity:  84.59243557714842\n",
      "Time:  8  secs\n",
      "Batch: 0\n",
      "Total loss:  8660.3515625\n",
      "Batch: 1\n",
      "Total loss:  17022.5615234375\n",
      "Batch: 2\n",
      "Total loss:  25205.77734375\n",
      "Batch: 3\n",
      "Total loss:  32710.9013671875\n",
      "Batch: 4\n",
      "Total loss:  40621.67041015625\n",
      "Epoch:  11 with perplexity:  68.81515528992415\n",
      "Time:  8  secs\n",
      "Batch: 0\n",
      "Total loss:  7964.47265625\n",
      "Batch: 1\n",
      "Total loss:  16573.9033203125\n",
      "Batch: 2\n",
      "Total loss:  24341.22265625\n",
      "Batch: 3\n",
      "Total loss:  32531.810546875\n",
      "Batch: 4\n",
      "Total loss:  40506.072265625\n",
      "Epoch:  12 with perplexity:  67.99148846227278\n",
      "Time:  7  secs\n",
      "Batch: 0\n",
      "Total loss:  7585.4599609375\n",
      "Batch: 1\n",
      "Total loss:  15662.96630859375\n",
      "Batch: 2\n",
      "Total loss:  23970.65771484375\n",
      "Batch: 3\n",
      "Total loss:  32137.716796875\n",
      "Batch: 4\n",
      "Total loss:  39802.197265625\n",
      "Epoch:  13 with perplexity:  63.18470201762846\n",
      "Time:  7  secs\n",
      "Batch: 0\n",
      "Total loss:  6926.7080078125\n",
      "Batch: 1\n",
      "Total loss:  14414.5341796875\n",
      "Batch: 2\n",
      "Total loss:  22210.267578125\n",
      "Batch: 3\n",
      "Total loss:  29814.91455078125\n",
      "Batch: 4\n",
      "Total loss:  37041.5341796875\n",
      "Epoch:  14 with perplexity:  47.39388176667269\n",
      "Time:  7  secs\n",
      "Batch: 0\n",
      "Total loss:  6675.130859375\n",
      "Batch: 1\n",
      "Total loss:  13714.63720703125\n",
      "Batch: 2\n",
      "Total loss:  20953.93701171875\n",
      "Batch: 3\n",
      "Total loss:  28355.3076171875\n",
      "Batch: 4\n",
      "Total loss:  35462.0419921875\n",
      "Epoch:  15 with perplexity:  40.20384780229844\n",
      "Time:  6  secs\n",
      "Batch: 0\n",
      "Total loss:  6796.83544921875\n",
      "Batch: 1\n",
      "Total loss:  13465.669921875\n",
      "Batch: 2\n",
      "Total loss:  20578.5146484375\n",
      "Batch: 3\n",
      "Total loss:  27918.6865234375\n",
      "Batch: 4\n",
      "Total loss:  35374.74609375\n",
      "Epoch:  16 with perplexity:  39.839918409724774\n",
      "Time:  7  secs\n",
      "Batch: 0\n",
      "Total loss:  6249.71142578125\n",
      "Batch: 1\n",
      "Total loss:  13289.943359375\n",
      "Batch: 2\n",
      "Total loss:  20659.5673828125\n",
      "Batch: 3\n",
      "Total loss:  27661.1484375\n",
      "Batch: 4\n",
      "Total loss:  34701.251953125\n",
      "Epoch:  17 with perplexity:  37.14071269345218\n",
      "Time:  6  secs\n",
      "Batch: 0\n",
      "Total loss:  7009.73046875\n",
      "Batch: 1\n",
      "Total loss:  13767.162109375\n",
      "Batch: 2\n",
      "Total loss:  21083.35205078125\n",
      "Batch: 3\n",
      "Total loss:  28635.03076171875\n",
      "Batch: 4\n",
      "Total loss:  35361.0966796875\n",
      "Epoch:  18 with perplexity:  39.78331370758538\n",
      "Time:  6  secs\n",
      "Batch: 0\n",
      "Total loss:  7556.146484375\n",
      "Batch: 1\n",
      "Total loss:  15272.8525390625\n",
      "Batch: 2\n",
      "Total loss:  23177.8916015625\n",
      "Batch: 3\n",
      "Total loss:  30840.4296875\n",
      "Batch: 4\n",
      "Total loss:  37576.91748046875\n",
      "Epoch:  19 with perplexity:  50.11208705366434\n",
      "Time:  6  secs\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def main():\n",
    "  \n",
    "  # get config execution\n",
    "  config = Config()\n",
    "  # get frequent words list \n",
    "  frequent_words = preprocess_helper.load_frequent_words(config.frequent_words_file_name)\n",
    "  print(\"frequent_words loaded\", len(frequent_words))\n",
    "  # construct hash map \n",
    "  vocab = {word: i for i,word in enumerate(frequent_words)}\n",
    "  # get testing and training data\n",
    "  #train_data, train_labels = preprocess_helper.load_and_process_data('data/sentences.train', frequent_words, 28)\n",
    "  all_train_data, all_train_labels = preprocess_helper.load_and_process_data(config.input_file_name,\n",
    "                                                                             vocab,\n",
    "                                                                             frequent_words,\n",
    "                                                                             28)\n",
    "  # compute number of batches\n",
    "  number_of_batches = int(len(all_train_data)/config.batch_size)\n",
    "  number_of_batches = 5\n",
    "  # init tensor flow session \n",
    "  session = tf.Session()\n",
    "\n",
    "  with session.as_default():\n",
    "    \n",
    "    # get the embedding matrix (with rand init emb or with word2vec)\n",
    "    with tf.variable_scope(\"Embedding\", reuse=tf.AUTO_REUSE):\n",
    "      embedding = tf.get_variable(\"embedding\",\n",
    "                                  [config.vocabulary_size, config.embedding_dimensions],\n",
    "                                  dtype=tf.float32)\n",
    "      load_embedding(session, vocab, embedding, config)\n",
    "      \n",
    "      # placeholder to get train_input and labels (batch_size x sent_len)\n",
    "      train_input_ph = tf.placeholder(tf.int64,\n",
    "                                      [config.batch_size, config.sentence_length],\n",
    "                                      name='train_input_ph')\n",
    "      train_labels_ph = tf.placeholder(tf.int64,\n",
    "                                      [config.batch_size, config.predicted_words],\n",
    "                                      name='train_labels_ph')\n",
    "\n",
    "      # generate 'usable' input_data for TF \n",
    "      embedded_train_input = tf.nn.embedding_lookup(embedding,\n",
    "                                                    train_input_ph)\n",
    "    \n",
    "    # Create a model instance \n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    with tf.variable_scope(\"Model\", reuse=tf.AUTO_REUSE, initializer=initializer):\n",
    "      # create an Model instance \n",
    "      model = Model(config)\n",
    "      # init \n",
    "      init = tf.global_variables_initializer()\n",
    "      session.run(init)\n",
    "\n",
    "      # loop over each epoch \n",
    "      for epoch_id in range(config.number_of_epochs):\n",
    "        # define perplexity and total_loss across all the batches\n",
    "        perplexity = 0\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        # loop over each batch \n",
    "        for batch_id in range(number_of_batches):\n",
    "          \n",
    "          # extract batch_size sentences from the training data \n",
    "          training_batch = all_train_data[batch_id*config.batch_size:(batch_id+1)*config.batch_size]\n",
    "          training_batch = session.run(embedded_train_input, {train_input_ph: training_batch})\n",
    "          \n",
    "          training_labels = all_train_labels[batch_id*config.batch_size:(batch_id+1)*config.batch_size,:]\n",
    "          \n",
    "          # variable to fect in the graph \n",
    "          fetches = {\n",
    "            \"loss\": model.loss,\n",
    "            \"probabilities\": model.probabilities,\n",
    "            \"train_optimizer\": model.train_optimizer\n",
    "          }\n",
    "\n",
    "          # input variables of the graph \n",
    "          feed_dict = {\n",
    "            \"Model/inputs:0\": training_batch,\n",
    "            \"Model/labels:0\": training_labels\n",
    "          }\n",
    "\n",
    "          # Feed the model with the training_batch and the training_labels \n",
    "          vals = session.run(fetches=fetches, feed_dict=feed_dict)\n",
    "                    \n",
    "          total_loss += vals[\"loss\"]\n",
    "          \n",
    "          print('Batch:', batch_id)\n",
    "          print('Total loss: ',total_loss)\n",
    "        \n",
    "        perplexity = np.exp(total_loss/float(config.sentence_length*number_of_batches*config.batch_size))\n",
    "        print('Epoch: ', epoch_id, 'with perplexity: ', perplexity)\n",
    "        print('Time: ', int(time.time()-start_time), ' secs')\n",
    "\n",
    "  if config.save_model:\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(session, config.saving_path)\n",
    "    if config.verbose:\n",
    "      print(\"Model saved in path: %s\" % save_path)\n",
    "    \n",
    "  # finally close the session ...\n",
    "  session.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow reshape playground to test built-in functions \n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "a = tf.constant([0, 0, 0, 0])\n",
    "b = tf.constant([1, 1, 1, 1])\n",
    "c = tf.constant([2, 2, 2, 2])\n",
    "\n",
    "list_of_tensor = [a,b,c]\n",
    "\n",
    "out = session.run(list_of_tensor)\n",
    "print(out)\n",
    "\n",
    "reshape_ = tf.reshape(list_of_tensor, [12])\n",
    "\n",
    "out = session.run(reshape_)\n",
    "print(out)\n",
    "\n",
    "reshape_reshape = tf.reshape(reshape_, [3,4])\n",
    "\n",
    "out = session.run(reshape_reshape)\n",
    "print(out)\n",
    "\n",
    "# display stuff from the graph in the back \n",
    "\n",
    "#variables_names = [v.name for v in tf.trainable_variables()]\n",
    "#values = session.run(variables_names)\n",
    "#for k, v in zip(variables_names, values):\n",
    "#    print(\"Variable: \", k)\n",
    "#    print(\"Shape: \", v.shape)\n",
    "#    print(v)\n",
    "\n",
    "\n",
    "#for op in tf.get_default_graph().get_operations():\n",
    "#  print(str(op.name))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
